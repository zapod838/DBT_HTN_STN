{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd292166-daaa-4443-89f1-15d41e7ab308",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting lightgbm\n  Using cached lightgbm-4.5.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.9/site-packages (from lightgbm) (1.7.3)\nRequirement already satisfied: numpy>=1.17.0 in /databricks/python3/lib/python3.9/site-packages (from lightgbm) (1.21.5)\nInstalling collected packages: lightgbm\nSuccessfully installed lightgbm-4.5.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5354b95e-5752-47fe-af5a-ebf0b10d8e14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting mlflow\n  Using cached mlflow-2.15.1-py3-none-any.whl (26.3 MB)\nCollecting sqlalchemy<3,>=1.4.0\n  Using cached SQLAlchemy-2.0.32-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\nCollecting graphene<4\n  Using cached graphene-3.3-py2.py3-none-any.whl (128 kB)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.7.3)\nCollecting gunicorn<23\n  Using cached gunicorn-22.0.0-py3-none-any.whl (84 kB)\nCollecting docker<8,>=4.0.0\n  Using cached docker-7.1.0-py3-none-any.whl (147 kB)\nRequirement already satisfied: numpy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.21.5)\nRequirement already satisfied: pyarrow<16,>=4.0.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (7.0.0)\nCollecting Flask<4\n  Using cached flask-3.0.3-py3-none-any.whl (101 kB)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.0.2)\nCollecting querystring-parser<2\n  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nCollecting markdown<4,>=3.3\n  Using cached Markdown-3.7-py3-none-any.whl (106 kB)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.4.2)\nCollecting mlflow-skinny==2.15.1\n  Using cached mlflow_skinny-2.15.1-py3-none-any.whl (5.5 MB)\nCollecting alembic!=1.10.0,<2\n  Using cached alembic-1.13.2-py3-none-any.whl (232 kB)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.5.1)\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2.11.3)\nCollecting gitpython<4,>=3.1.9\n  Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\nCollecting sqlparse<1,>=0.4.0\n  Using cached sqlparse-0.5.1-py3-none-any.whl (44 kB)\nCollecting importlib-metadata!=4.7.0,<8,>=3.7.0\n  Using cached importlib_metadata-7.2.1-py3-none-any.whl (25 kB)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.15.1->mlflow) (2.27.1)\nCollecting opentelemetry-api<3,>=1.9.0\n  Using cached opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.15.1->mlflow) (8.0.4)\nCollecting opentelemetry-sdk<3,>=1.9.0\n  Using cached opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\nRequirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.15.1->mlflow) (0.4)\nRequirement already satisfied: protobuf<6,>=3.12.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.15.1->mlflow) (3.19.4)\nCollecting cloudpickle<4\n  Using cached cloudpickle-3.0.0-py3-none-any.whl (20 kB)\nRequirement already satisfied: pytz<2025 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.15.1->mlflow) (2021.3)\nCollecting pyyaml<7,>=5.1\n  Using cached PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (737 kB)\nCollecting databricks-sdk<1,>=0.20.0\n  Using cached databricks_sdk-0.30.0-py3-none-any.whl (538 kB)\nCollecting cachetools<6,>=5.0.0\n  Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\nRequirement already satisfied: packaging<25 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==2.15.1->mlflow) (21.3)\nRequirement already satisfied: typing-extensions>=4 in /databricks/python3/lib/python3.9/site-packages (from alembic!=1.10.0,<2->mlflow) (4.1.1)\nCollecting Mako\n  Using cached Mako-1.3.5-py3-none-any.whl (78 kB)\nCollecting google-auth~=2.0\n  Using cached google_auth-2.34.0-py2.py3-none-any.whl (200 kB)\nCollecting requests<3,>=2.17.3\n  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.9/site-packages (from docker<8,>=4.0.0->mlflow) (1.26.9)\nCollecting blinker>=1.6.2\n  Using cached blinker-1.8.2-py3-none-any.whl (9.5 kB)\nCollecting Jinja2<4,>=2.11\n  Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\nCollecting Werkzeug>=3.0.0\n  Using cached werkzeug-3.0.3-py3-none-any.whl (227 kB)\nCollecting itsdangerous>=2.1.2\n  Using cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\nCollecting click<9,>=7.0\n  Using cached click-8.1.7-py3-none-any.whl (97 kB)\nCollecting gitdb<5,>=4.0.1\n  Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\nCollecting smmap<6,>=3.0.1\n  Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\nCollecting rsa<5,>=3.1.4\n  Using cached rsa-4.9-py3-none-any.whl (34 kB)\nCollecting pyasn1-modules>=0.2.1\n  Using cached pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\nCollecting graphql-core<3.3,>=3.1\n  Using cached graphql_core-3.2.3-py3-none-any.whl (202 kB)\nCollecting graphql-relay<3.3,>=3.1\n  Using cached graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nCollecting aniso8601<10,>=8\n  Using cached aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\nCollecting zipp>=0.5\n  Using cached zipp-3.20.0-py3-none-any.whl (9.4 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.9/site-packages (from Jinja2<4,>=2.11->mlflow) (2.0.1)\nRequirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (3.0.4)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (0.11.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (1.3.2)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (9.0.1)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (2.8.2)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (4.25.0)\nCollecting deprecated>=1.2.6\n  Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\nCollecting wrapt<2,>=1.10\n  Using cached wrapt-1.16.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\nCollecting opentelemetry-semantic-conventions==0.47b0\n  Using cached opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\nCollecting pyasn1<0.7.0,>=0.4.6\n  Using cached pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib<4->mlflow) (1.16.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.15.1->mlflow) (2021.10.8)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.15.1->mlflow) (3.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.15.1->mlflow) (2.0.4)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (1.1.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (2.2.0)\nCollecting typing-extensions>=4\n  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nCollecting greenlet!=0.4.17\n  Using cached greenlet-3.0.3-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (614 kB)\nCollecting MarkupSafe>=2.0\n  Using cached MarkupSafe-2.1.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\nInstalling collected packages: zipp, wrapt, pyasn1, importlib-metadata, deprecated, smmap, rsa, pyasn1-modules, opentelemetry-api, cachetools, typing-extensions, requests, opentelemetry-semantic-conventions, MarkupSafe, greenlet, graphql-core, google-auth, gitdb, Werkzeug, sqlparse, sqlalchemy, pyyaml, opentelemetry-sdk, Mako, Jinja2, itsdangerous, graphql-relay, gitpython, databricks-sdk, cloudpickle, click, blinker, aniso8601, querystring-parser, mlflow-skinny, markdown, gunicorn, graphene, Flask, docker, alembic, mlflow\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 4.1.1\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-c38f5022-cba1-460c-b1bb-16538304ab81\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\n  Attempting uninstall: requests\n    Found existing installation: requests 2.27.1\n    Not uninstalling requests at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-c38f5022-cba1-460c-b1bb-16538304ab81\n    Can't uninstall 'requests'. No files were found to uninstall.\n  Attempting uninstall: MarkupSafe\n    Found existing installation: MarkupSafe 2.0.1\n    Not uninstalling markupsafe at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-c38f5022-cba1-460c-b1bb-16538304ab81\n    Can't uninstall 'MarkupSafe'. No files were found to uninstall.\n  Attempting uninstall: Jinja2\n    Found existing installation: Jinja2 2.11.3\n    Not uninstalling jinja2 at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-c38f5022-cba1-460c-b1bb-16538304ab81\n    Can't uninstall 'Jinja2'. No files were found to uninstall.\n  Attempting uninstall: click\n    Found existing installation: click 8.0.4\n    Not uninstalling click at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-c38f5022-cba1-460c-b1bb-16538304ab81\n    Can't uninstall 'click'. No files were found to uninstall.\nSuccessfully installed Flask-3.0.3 Jinja2-3.1.4 Mako-1.3.5 MarkupSafe-2.1.5 Werkzeug-3.0.3 alembic-1.13.2 aniso8601-9.0.1 blinker-1.8.2 cachetools-5.5.0 click-8.1.7 cloudpickle-3.0.0 databricks-sdk-0.30.0 deprecated-1.2.14 docker-7.1.0 gitdb-4.0.11 gitpython-3.1.43 google-auth-2.34.0 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 greenlet-3.0.3 gunicorn-22.0.0 importlib-metadata-7.2.1 itsdangerous-2.2.0 markdown-3.7 mlflow-2.15.1 mlflow-skinny-2.15.1 opentelemetry-api-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 pyasn1-0.6.0 pyasn1-modules-0.4.0 pyyaml-6.0.2 querystring-parser-1.2.4 requests-2.32.3 rsa-4.9 smmap-5.0.1 sqlalchemy-2.0.32 sqlparse-0.5.1 typing-extensions-4.12.2 wrapt-1.16.0 zipp-3.20.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d30b2268-c330-46e0-8815-12d4f4b3e556",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting xgboost\n  Using cached xgboost-2.1.1-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\nCollecting nvidia-nccl-cu12\n  Using cached nvidia_nccl_cu12-2.22.3-py3-none-manylinux2014_x86_64.whl (190.9 MB)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.9/site-packages (from xgboost) (1.7.3)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.9/site-packages (from xgboost) (1.21.5)\nInstalling collected packages: nvidia-nccl-cu12, xgboost\nSuccessfully installed nvidia-nccl-cu12-2.22.3 xgboost-2.1.1\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df625691-a30e-4ecd-a56f-170145554a1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting tensorflow\n  Using cached tensorflow-2.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\nCollecting keras>=3.2.0\n  Using cached keras-3.5.0-py3-none-any.whl (1.1 MB)\nCollecting tensorflow-io-gcs-filesystem>=0.23.1\n  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\nCollecting numpy<2.0.0,>=1.23.5\n  Using cached numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\nRequirement already satisfied: typing-extensions>=3.6.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c38f5022-cba1-460c-b1bb-16538304ab81/lib/python3.9/site-packages (from tensorflow) (4.12.2)\nRequirement already satisfied: setuptools in /databricks/python3/lib/python3.9/site-packages (from tensorflow) (61.2.0)\nCollecting absl-py>=1.0.0\n  Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\nRequirement already satisfied: wrapt>=1.11.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c38f5022-cba1-460c-b1bb-16538304ab81/lib/python3.9/site-packages (from tensorflow) (1.16.0)\nCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n  Using cached protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\nCollecting opt-einsum>=2.3.2\n  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\nCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n  Using cached gast-0.6.0-py3-none-any.whl (21 kB)\nCollecting tensorboard<2.18,>=2.17\n  Using cached tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\nCollecting termcolor>=1.1.0\n  Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\nCollecting ml-dtypes<0.5.0,>=0.3.1\n  Using cached ml_dtypes-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\nCollecting astunparse>=1.6.0\n  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nCollecting h5py>=3.10.0\n  Using cached h5py-3.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\nCollecting grpcio<2.0,>=1.24.3\n  Using cached grpcio-1.65.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\nRequirement already satisfied: requests<3,>=2.21.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c38f5022-cba1-460c-b1bb-16538304ab81/lib/python3.9/site-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.9/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: six>=1.12.0 in /databricks/python3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\nCollecting google-pasta>=0.1.1\n  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\nCollecting flatbuffers>=24.3.25\n  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\nCollecting libclang>=13.0.0\n  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /databricks/python3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\nCollecting namex\n  Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\nCollecting optree\n  Using cached optree-0.12.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (347 kB)\nCollecting rich\n  Using cached rich-13.7.1-py3-none-any.whl (240 kB)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.9)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (2021.10.8)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\nCollecting tensorboard-data-server<0.8.0,>=0.7.0\n  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\nRequirement already satisfied: werkzeug>=1.0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c38f5022-cba1-460c-b1bb-16538304ab81/lib/python3.9/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\nRequirement already satisfied: markdown>=2.6.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c38f5022-cba1-460c-b1bb-16538304ab81/lib/python3.9/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\nRequirement already satisfied: importlib-metadata>=4.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c38f5022-cba1-460c-b1bb-16538304ab81/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.18,>=2.17->tensorflow) (7.2.1)\nRequirement already satisfied: zipp>=0.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c38f5022-cba1-460c-b1bb-16538304ab81/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.18,>=2.17->tensorflow) (3.20.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c38f5022-cba1-460c-b1bb-16538304ab81/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.4)\nCollecting pygments<3.0.0,>=2.13.0\n  Using cached pygments-2.18.0-py3-none-any.whl (1.2 MB)\nCollecting markdown-it-py>=2.2.0\n  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\nCollecting mdurl~=0.1\n  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nInstalling collected packages: mdurl, pygments, numpy, markdown-it-py, tensorboard-data-server, rich, protobuf, optree, namex, ml-dtypes, h5py, grpcio, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n  Attempting uninstall: pygments\n    Found existing installation: Pygments 2.11.2\n    Not uninstalling pygments at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-c38f5022-cba1-460c-b1bb-16538304ab81\n    Can't uninstall 'Pygments'. No files were found to uninstall.\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.5\n    Not uninstalling numpy at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-c38f5022-cba1-460c-b1bb-16538304ab81\n    Can't uninstall 'numpy'. No files were found to uninstall.\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.19.4\n    Not uninstalling protobuf at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-c38f5022-cba1-460c-b1bb-16538304ab81\n    Can't uninstall 'protobuf'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nscipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.26.4 which is incompatible.\nSuccessfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.65.5 h5py-3.11.0 keras-3.5.0 libclang-18.1.1 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.0 namex-0.0.8 numpy-1.26.4 opt-einsum-3.3.0 optree-0.12.1 protobuf-4.25.4 pygments-2.18.0 rich-13.7.1 tensorboard-2.17.1 tensorboard-data-server-0.7.2 tensorflow-2.17.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.4.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35a0b604-57b3-4532-964e-42c4742e32cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/dbfs/FileStore/processed_data/DBT_df_encoded.csv/</td><td>DBT_df_encoded.csv/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/dbfs/FileStore/processed_data/HTN_df_encoded.csv/</td><td>HTN_df_encoded.csv/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/dbfs/FileStore/processed_data/STN_df_encoded.csv/</td><td>STN_df_encoded.csv/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/dbfs/FileStore/processed_data/STN_df_encoded.parquet/</td><td>STN_df_encoded.parquet/</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/dbfs/FileStore/processed_data/DBT_df_encoded.csv/",
         "DBT_df_encoded.csv/",
         0,
         0
        ],
        [
         "dbfs:/dbfs/FileStore/processed_data/HTN_df_encoded.csv/",
         "HTN_df_encoded.csv/",
         0,
         0
        ],
        [
         "dbfs:/dbfs/FileStore/processed_data/STN_df_encoded.csv/",
         "STN_df_encoded.csv/",
         0,
         0
        ],
        [
         "dbfs:/dbfs/FileStore/processed_data/STN_df_encoded.parquet/",
         "STN_df_encoded.parquet/",
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls dbfs/FileStore/processed_data/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94d6f5fb-8d92-4044-a7c5-431085b455f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PAT_ID</th>\n",
       "      <th>mpr</th>\n",
       "      <th>mpr80</th>\n",
       "      <th>MAX_GAP_IND</th>\n",
       "      <th>DS_nm</th>\n",
       "      <th>fills_nm</th>\n",
       "      <th>claim_90D_nm</th>\n",
       "      <th>claims_nm</th>\n",
       "      <th>percent_90D_nm</th>\n",
       "      <th>gpi10_cnt_nm</th>\n",
       "      <th>...</th>\n",
       "      <th>DC_DIA</th>\n",
       "      <th>gndr_cd_M</th>\n",
       "      <th>PREDOMINANT_RACE_BLACK</th>\n",
       "      <th>PREDOMINANT_RACE_HISPANIC</th>\n",
       "      <th>PREDOMINANT_RACE_OTHER RACE</th>\n",
       "      <th>PREDOMINANT_RACE_WHITE</th>\n",
       "      <th>URBANICITY_RURAL</th>\n",
       "      <th>URBANICITY_SUBURBAN</th>\n",
       "      <th>URBANICITY_URBAN</th>\n",
       "      <th>URBANICITY__Missing_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.094306e+10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>360</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.094306e+10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.094307e+10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>270</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.094410e+10</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.094411e+10</td>\n",
       "      <td>0.967532</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PAT_ID</th>\n      <th>mpr</th>\n      <th>mpr80</th>\n      <th>MAX_GAP_IND</th>\n      <th>DS_nm</th>\n      <th>fills_nm</th>\n      <th>claim_90D_nm</th>\n      <th>claims_nm</th>\n      <th>percent_90D_nm</th>\n      <th>gpi10_cnt_nm</th>\n      <th>...</th>\n      <th>DC_DIA</th>\n      <th>gndr_cd_M</th>\n      <th>PREDOMINANT_RACE_BLACK</th>\n      <th>PREDOMINANT_RACE_HISPANIC</th>\n      <th>PREDOMINANT_RACE_OTHER RACE</th>\n      <th>PREDOMINANT_RACE_WHITE</th>\n      <th>URBANICITY_RURAL</th>\n      <th>URBANICITY_SUBURBAN</th>\n      <th>URBANICITY_URBAN</th>\n      <th>URBANICITY__Missing_</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.094306e+10</td>\n      <td>1.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>360</td>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.094306e+10</td>\n      <td>1.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>180</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.094307e+10</td>\n      <td>1.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>270</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.094410e+10</td>\n      <td>0.642857</td>\n      <td>0</td>\n      <td>0</td>\n      <td>90</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.094411e+10</td>\n      <td>0.967532</td>\n      <td>1</td>\n      <td>0</td>\n      <td>180</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 87 columns</p>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_path_csv = '/dbfs/FileStore/processed_data/DBT_df_encoded.csv'\n",
    "\n",
    "# Load the CSV file into a Spark DataFrame\n",
    "DBT_spark_df_loaded_csv = spark.read.csv(input_path_csv, header=True, inferSchema=True)\n",
    "\n",
    "# Convert the Spark DataFrame to a Pandas DataFrame\n",
    "DBT_df_encoded = DBT_spark_df_loaded_csv.toPandas()\n",
    "\n",
    "# Display the Pandas DataFrame\n",
    "DBT_df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "469dfb9c-18ab-48ec-93c5-f0066975ad49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Predicting Medication Adherence\n",
    "\n",
    "The primary goal of the model is to predict whether a patient is likely to adhere to their medication regimen. \n",
    "\n",
    "This is based on the mpr80 column, where:\n",
    "\n",
    "• mpr80 = 1: Indicates that the patient's Medication Possession Ratio (MPR) is 80% or higher, meaning they are adherent to their medication.\n",
    "\n",
    "• mpr80 = 0: Indicates that the patient's MPR is below 80%, meaning they are not adherent to their medication.\n",
    "\n",
    "Identifying patients who are at high risk of non-adherence (i.e., those predicted to have mpr80 = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "647d1484-3476-426f-8223-fb06579c4f8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [\"DBT_df_encoded\"], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Adding new features\n",
    "DBT_df_encoded['total_gaps'] = DBT_df_encoded['gaps_bl'] + DBT_df_encoded['leading_gap']\n",
    "DBT_df_encoded['high_copay'] = (DBT_df_encoded['copay_tc_nm'] > DBT_df_encoded['copay_tc_nm'].mean()).astype(int)\n",
    "\n",
    "# Step 4: Define target and features\n",
    "X = DBT_df_encoded.drop(columns=['mpr', 'mpr80', 'PAT_ID'])  # Drop unnecessary columns including mpr and mpr80\n",
    "y = DBT_df_encoded['mpr80']  # Use mpr80 as the target variable\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e1f15a8-5105-4455-83c2-09bba78b9e5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45169bdd-2253-417b-8240-2ae30d54dd7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n2024/08/20 15:44:22 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.15.1/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model.\n2024/08/20 15:44:23 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n2024/08/20 15:44:23 INFO mlflow.tracking._tracking_service.client: 🏃 View run XGBoost Classifier DBT at: https://community.cloud.databricks.com/ml/experiments/2051286152915315/runs/079bae5c22714785a47ff0a3acb4a0e9.\n2024/08/20 15:44:23 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://community.cloud.databricks.com/ml/experiments/2051286152915315.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - Accuracy: 0.97, Precision: 0.99, Recall: 0.98, F1 Score: 0.99, ROC AUC Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"XGBoost Classifier DBT\"):\n",
    "    # Initialize XGBoost Classifier\n",
    "    xgb_model = XGBClassifier(random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting on test set\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "    \n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(xgb_model, \"model\")\n",
    "\n",
    "    print(f\"XGBoost - Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}, ROC AUC Score: {roc_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "465542a9-401a-40fd-a7e7-4fc1ee71c9e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 35510, number of negative: 3529\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014682 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5353\n[LightGBM] [Info] Number of data points in the train set: 39039, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.909603 -> initscore=2.308800\n[LightGBM] [Info] Start training from score 2.308800\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/20 15:44:29 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.15.1/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model.\n2024/08/20 15:44:30 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n2024/08/20 15:44:30 INFO mlflow.tracking._tracking_service.client: 🏃 View run LightGBM Classifier DBT at: https://community.cloud.databricks.com/ml/experiments/2051286152915315/runs/5f64a3d0579f422dad1448ca59129c3a.\n2024/08/20 15:44:30 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://community.cloud.databricks.com/ml/experiments/2051286152915315.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM - Accuracy: 0.98, Precision: 0.99, Recall: 0.98, F1 Score: 0.99, ROC AUC Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"LightGBM Classifier DBT\"):\n",
    "    # Initialize LGBM Classifier\n",
    "    lgbm_model = LGBMClassifier(random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    lgbm_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting on test set\n",
    "    y_pred = lgbm_model.predict(X_test)\n",
    "    y_pred_proba = lgbm_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "    \n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(lgbm_model, \"model\")\n",
    "\n",
    "    print(f\"LGBM - Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}, ROC AUC Score: {roc_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71215f86-6435-41f7-a2e1-758598ba119f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n2024/08/20 15:44:47 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.15.1/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model.\n2024/08/20 15:44:48 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n2024/08/20 15:44:48 INFO mlflow.tracking._tracking_service.client: 🏃 View run Logistic Regression DBT at: https://community.cloud.databricks.com/ml/experiments/2051286152915315/runs/9d43d127c79d46358a6734447b2e757d.\n2024/08/20 15:44:48 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://community.cloud.databricks.com/ml/experiments/2051286152915315.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Accuracy: 0.91, Precision: 0.91, Recall: 1.00, F1 Score: 0.95, ROC AUC Score: 0.83\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"Logistic Regression DBT\"):\n",
    "    # Initialize Logistic Regression\n",
    "    logreg_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    \n",
    "    # Train the model\n",
    "    logreg_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting on test set\n",
    "    y_pred = logreg_model.predict(X_test)\n",
    "    y_pred_proba = logreg_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "    \n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(logreg_model, \"model\")\n",
    "\n",
    "    print(f\"Logistic Regression - Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}, ROC AUC Score: {roc_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b6f2bba-a76a-4afc-a25e-8793f039c33c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/20 15:44:58 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.15.1/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model.\n2024/08/20 15:44:59 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Accuracy: 0.98, Precision: 1.00, Recall: 0.98, F1 Score: 0.99, ROC AUC Score: 0.99\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/20 15:45:00 INFO mlflow.tracking._tracking_service.client: 🏃 View run Random Forest Classifier DBT at: https://community.cloud.databricks.com/ml/experiments/2051286152915315/runs/eaa15b1ddab94864821534c4427526e6.\n2024/08/20 15:45:00 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://community.cloud.databricks.com/ml/experiments/2051286152915315.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"Random Forest Classifier DBT\"):\n",
    "    # Initialize RandomForestClassifier\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting on test set\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "    \n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(rf_model, \"model\")\n",
    "\n",
    "    print(f\"Random Forest - Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}, ROC AUC Score: {roc_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e06e833-a8fc-4460-9c98-d75b78478e14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-c38f5022-cba1-460c-b1bb-16538304ab81/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n976/976 - 4s - 4ms/step - accuracy: 0.8388 - auc: 0.5012 - loss: 425.1530 - val_accuracy: 0.9097 - val_auc: 0.4968 - val_loss: 0.7417\nEpoch 2/10\n976/976 - 2s - 2ms/step - accuracy: 0.8974 - auc: 0.4981 - loss: 3.4506 - val_accuracy: 0.9097 - val_auc: 0.4987 - val_loss: 0.3462\nEpoch 3/10\n976/976 - 3s - 3ms/step - accuracy: 0.9076 - auc: 0.4954 - loss: 0.7620 - val_accuracy: 0.9097 - val_auc: 0.4987 - val_loss: 0.3147\nEpoch 4/10\n976/976 - 2s - 3ms/step - accuracy: 0.9085 - auc: 0.5028 - loss: 0.5739 - val_accuracy: 0.9097 - val_auc: 0.5001 - val_loss: 0.3039\nEpoch 5/10\n976/976 - 2s - 3ms/step - accuracy: 0.9093 - auc: 0.4930 - loss: 0.4975 - val_accuracy: 0.9097 - val_auc: 0.5001 - val_loss: 0.3033\nEpoch 6/10\n976/976 - 2s - 2ms/step - accuracy: 0.9092 - auc: 0.4982 - loss: 0.3996 - val_accuracy: 0.9097 - val_auc: 0.5001 - val_loss: 0.3032\nEpoch 7/10\n976/976 - 2s - 2ms/step - accuracy: 0.9094 - auc: 0.4991 - loss: 0.3813 - val_accuracy: 0.9097 - val_auc: 0.5001 - val_loss: 0.3032\nEpoch 8/10\n976/976 - 2s - 2ms/step - accuracy: 0.9094 - auc: 0.4987 - loss: 0.3081 - val_accuracy: 0.9097 - val_auc: 0.5001 - val_loss: 0.3032\nEpoch 9/10\n976/976 - 2s - 2ms/step - accuracy: 0.9095 - auc: 0.4929 - loss: 0.3297 - val_accuracy: 0.9097 - val_auc: 0.5001 - val_loss: 0.3032\nEpoch 10/10\n976/976 - 3s - 3ms/step - accuracy: 0.9095 - auc: 0.4956 - loss: 0.3083 - val_accuracy: 0.9097 - val_auc: 0.5001 - val_loss: 0.3032\n\r\u001B[1m  1/523\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m29s\u001B[0m 57ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m 38/523\u001B[0m \u001B[32m━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 1ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m 73/523\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 1ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m118/523\u001B[0m \u001B[32m━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 1ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m150/523\u001B[0m \u001B[32m━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 1ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m179/523\u001B[0m \u001B[32m━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 1ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m205/523\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m233/523\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m262/523\u001B[0m \u001B[32m━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m291/523\u001B[0m \u001B[32m━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m319/523\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m348/523\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m376/523\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m405/523\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m434/523\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m464/523\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m498/523\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 2ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m523/523\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m523/523\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/20 15:45:33 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n2024/08/20 15:45:39 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.15.1/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model.\n2024/08/20 15:45:40 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n2024/08/20 15:45:40 INFO mlflow.tracking._tracking_service.client: 🏃 View run Neural Network Classifier DBT at: https://community.cloud.databricks.com/ml/experiments/2051286152915315/runs/4e293429a96c45db8acbab8ef21566f4.\n2024/08/20 15:45:40 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://community.cloud.databricks.com/ml/experiments/2051286152915315.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network - Accuracy: 0.91, Precision: 0.91, Recall: 1.00, F1 Score: 0.95, ROC AUC Score: 0.50\n"
     ]
    }
   ],
   "source": [
    "import mlflow.tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"Neural Network Classifier DBT\"):\n",
    "    # Define the neural network architecture\n",
    "    model = Sequential([\n",
    "        Dense(64, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss=BinaryCrossentropy(),\n",
    "                  metrics=['accuracy', AUC(name='auc')])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        epochs=10, \n",
    "                        batch_size=32, \n",
    "                        validation_split=0.2,\n",
    "                        verbose=2)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_accuracy, test_auc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(\"int32\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", test_accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    mlflow.log_metric(\"roc_auc\", test_auc)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.tensorflow.log_model(model, \"model\")\n",
    "\n",
    "    print(f\"Neural Network - Accuracy: {test_accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}, ROC AUC Score: {test_auc:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2051286152915321,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DBT_Models",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
